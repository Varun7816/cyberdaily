<!DOCTYPE html><!--HFWSpj3XGqc8o6Oyln5f6--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/bf6a6db65f82309d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/6420740671896b80.js"/><script src="/_next/static/chunks/2af9fca721db194b.js" async=""></script><script src="/_next/static/chunks/63dba52cde864d84.js" async=""></script><script src="/_next/static/chunks/8082ab48faca5ea1.js" async=""></script><script src="/_next/static/chunks/turbopack-aa628873bc85b21e.js" async=""></script><script src="/_next/static/chunks/0362750dd751a011.js" async=""></script><script src="/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/_next/static/chunks/7dd66bdf8a7e5707.js" async=""></script><link rel="icon" href="/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><header class="bg-white border-b border-gray-200 sticky top-0 z-50"><div class="max-w-7xl mx-auto flex items-center justify-between h-16 px-4"><div class="flex items-center gap-3"><span class="w-10 h-10 rounded bg-blue-600 text-white flex items-center justify-center font-bold text-xl">CT</span><span class="text-3xl font-extrabold text-gray-900 tracking-tight font-merriweather">CyberTimes</span></div><span class="flex items-center gap-1 bg-blue-600 text-white font-semibold cursor-pointer px-4 py-2 rounded-lg shadow ml-auto transition hover:bg-blue-700"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail w-5 h-5" aria-hidden="true"><path d="m22 7-8.991 5.727a2 2 0 0 1-2.009 0L2 7"></path><rect x="2" y="4" width="20" height="16" rx="2"></rect></svg><span>Subscribe - Get latest news</span></span></div><nav class="bg-white border-t border-b border-gray-200"><div class="max-w-7xl mx-auto flex gap-10 px-4 py-4 overflow-x-auto whitespace-nowrap scrollbar-hide"><a class="text-gray-700 font-semibold hover:text-blue-600 transition-colors text-sm" href="/">Home</a><a class="text-gray-700 font-semibold hover:text-blue-600 transition-colors text-sm" href="/cyber-incidents">Cyber Incidents</a><a class="text-gray-700 font-semibold hover:text-blue-600 transition-colors text-sm" href="/vulnerabilities">Vulnerabilities</a><a class="text-gray-700 font-semibold hover:text-blue-600 transition-colors text-sm" href="/zero-days">Zero-Days / Exploits</a><a class="text-gray-700 font-semibold hover:text-blue-600 transition-colors text-sm" href="/malware">Malware Alerts</a><a class="text-gray-700 font-semibold hover:text-blue-600 transition-colors text-sm" href="/jobs">Jobs</a><a href="#" class="text-gray-700 font-semibold hover:text-blue-600 transition-colors text-sm">Events</a><a href="#" class="text-gray-700 font-semibold hover:text-blue-600 transition-colors text-sm">Resources</a></div></nav></header><main class="min-h-screen bg-gray-50 py-10 px-4 sm:px-6"><div class="max-w-3xl mx-auto bg-white border border-gray-200 rounded-2xl shadow-lg p-8 md:p-10"><h1 class="text-3xl font-bold text-gray-900 mb-3 leading-snug">The Risks of Code Assistant LLMs: Prompt Injection, Backdoors &amp; Safe Use</h1><p class="text-gray-700 mb-4 leading-relaxed text-justify">Explore how code assistants (LLMs) can be abused ‚Äî from indirect prompt injection to hidden backdoors ‚Äî and learn best practices to protect your dev environment.</p><div class="flex items-center text-xs text-gray-500 font-medium mb-6 gap-2"><span>üóìÔ∏è <!-- -->15 September 2025</span></div><h2 class="text-2xl font-semibold text-gray-900 mb-6">The Risks of Code Assistant LLMs: Prompt Injection, Backdoors &amp; Safe Use</h2><section class="mb-8"><h3 class="text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4">Executive Summary</h3><p class="mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line">AI-powered code assistants‚Äîthose integrated with IDEs to autocomplete, suggest, or refactor code‚Äîcan boost developer productivity. But they also introduce serious risks: prompt injection, hidden backdoors, and unintended code execution. This article examines how threat actors can exploit them and offers guidance for safely using AI coding tools.</p></section><section class="mb-8"><h3 class="text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4">Introduction: The Rise and Risk of LLM Code Assistants</h3><p class="mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line">AI code assistants (e.g. GitHub Copilot style tools) are increasingly embedded into development workflows, offering suggestions, rewriting code, generating tests, and more. But while they help speed development, they also open new attack surfaces. Components like context attachment, auto-complete, and direct model invocation can be manipulated maliciously.</p></section><section class="mb-8"><h3 class="text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4">Prompt Injection: The Hidden Threat</h3><p class="mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line">‚Ä¢ Direct Prompt Injection: Attackers craft inputs that override system instructions inside the same prompt structure.
‚Ä¢ Indirect Prompt Injection: Threat actors insert malicious prompts into external data sources (e.g. public repos, APIs). When a developer feeds that data as ‚Äúcontext‚Äù to a coding assistant, the embedded prompt hijacks the assistant.
‚Ä¢ Because LLMs treat all text uniformly, malicious instructions camouflaged inside data can override logic.</p></section><section class="mb-8"><h3 class="text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4">Misuse of Context Attachment</h3><p class="mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line">Many assistants let developers attach external files, URLs, or repositories as context. This boosts relevance but also means if that external content is manipulated, the LLM can be manipulated too. Users may inadvertently feed tainted data, enabling stealthy prompt injection.</p></section><section class="mb-8"><h3 class="text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4">Harmful Content &amp; Auto-Completion Abuse</h3><p class="mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line">Auto-completion is a double-edged sword. If a prompt or prefix is crafted cleverly, it can coax the model to generate harmful or insecure code‚Äîe.g. insert backdoors, open remote connections, or include data exfiltration logic.</p></section><section class="mb-8"><h3 class="text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4">Direct Model Invocation &amp; Token Hijacking</h3><p class="mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line">Some assistants allow direct calls to the base model from the client side, bypassing constraints. Combined with stolen session tokens (a scenario we call **LLMJacking**), adversaries can supply new system prompts and produce malicious outputs. They may even rent or resell access to legitimate model endpoints.</p></section><section class="mb-8"><h3 class="text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4">Mitigation Strategies &amp; Safe Practices</h3><p class="mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line">1. Always Review Generated Code ‚Äî Never blindly trust an AI assistant. Use code reviews, static analysis, and manual auditing before executing suggestions.
2. Sanitize &amp; Vet Context Sources ‚Äî Before attaching external files, repositories, or URLs as context, vet their integrity. Avoid unknown or untrusted sources.
3. Limit Auto-Completion in Sensitive Modules ‚Äî Disable or restrict autocomplete features when working in cryptography, authentication, or networking modules.
4. Enforce Usage Boundaries ‚Äî Use prompts that clearly separate system instructions from user inputs. Set usage policies for dynamic system prompts.
5. Token &amp; Access Protection ‚Äî Secure session tokens and API keys. Ensure they‚Äôre not exposed to client-side code, and rotate them frequently to mitigate misuse.
6. Adopt Defense-in-Depth for AI Tools ‚Äî Combine sandboxing, runtime checks, and anomaly detection to monitor for unexpected behavior triggered by AI outputs.</p></section><section class="mb-8"><h3 class="text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4">Conclusion &amp; Future Outlook</h3><p class="mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line">As AI coding assistants become more powerful and ubiquitous, attackers will continue finding new vectors‚Äîespecially via prompt injection and context manipulation. The balance between convenience and security is delicate. Developers must remain vigilant, continuously refine their practices, and never forget: the human in the loop is the last line of defense.</p></section><div class="mt-10 pt-4 border-t text-sm text-gray-500"><p class="mb-2">Original research and full technical detail available at Unit 42‚Äôs site</p><a href="https://unit42.paloaltonetworks.com/code-assistant-llms/" target="_blank" rel="noopener noreferrer" class="text-blue-700 underline hover:text-blue-900">https://unit42.paloaltonetworks.com/code-assistant-llms/</a></div></div></main><!--$--><!--/$--><script src="/_next/static/chunks/6420740671896b80.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[53243,[\"/_next/static/chunks/0362750dd751a011.js\"],\"default\"]\n3:I[3470,[\"/_next/static/chunks/0362750dd751a011.js\"],\"default\"]\n4:I[39756,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7dd66bdf8a7e5707.js\"],\"default\"]\n5:I[37457,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7dd66bdf8a7e5707.js\"],\"default\"]\n10:I[68027,[\"/_next/static/chunks/0362750dd751a011.js\"],\"default\"]\n:HL[\"/_next/static/chunks/bf6a6db65f82309d.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"HFWSpj3XGqc8o6Oyln5f6\",\"p\":\"\",\"c\":[\"\",\"malware\",\"risks-code-assistant-llms\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"malware\",{\"children\":[[\"slug\",\"risks-code-assistant-llms\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/bf6a6db65f82309d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/0362750dd751a011.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{}],[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]}]}]]}],{\"children\":[\"malware\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"risks-code-assistant-llms\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"main\",null,{\"className\":\"min-h-screen bg-gray-50 py-10 px-4 sm:px-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-3xl mx-auto bg-white border border-gray-200 rounded-2xl shadow-lg p-8 md:p-10\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold text-gray-900 mb-3 leading-snug\",\"children\":\"The Risks of Code Assistant LLMs: Prompt Injection, Backdoors \u0026 Safe Use\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 mb-4 leading-relaxed text-justify\",\"children\":\"Explore how code assistants (LLMs) can be abused ‚Äî from indirect prompt injection to hidden backdoors ‚Äî and learn best practices to protect your dev environment.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center text-xs text-gray-500 font-medium mb-6 gap-2\",\"children\":[\"$\",\"span\",null,{\"children\":[\"üóìÔ∏è \",\"15 September 2025\"]}]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-6\",\"children\":\"The Risks of Code Assistant LLMs: Prompt Injection, Backdoors \u0026 Safe Use\"}],[[\"$\",\"section\",\"0\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4\",\"children\":\"Executive Summary\"}],[\"$\",\"p\",null,{\"className\":\"mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line\",\"children\":\"AI-powered code assistants‚Äîthose integrated with IDEs to autocomplete, suggest, or refactor code‚Äîcan boost developer productivity. But they also introduce serious risks: prompt injection, hidden backdoors, and unintended code execution. This article examines how threat actors can exploit them and offers guidance for safely using AI coding tools.\"}],[]]}],[\"$\",\"section\",\"1\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4\",\"children\":\"Introduction: The Rise and Risk of LLM Code Assistants\"}],\"$L6\",[]]}],\"$L7\",\"$L8\",\"$L9\",\"$La\",\"$Lb\",\"$Lc\"],\"$undefined\",\"$undefined\",false,\"$Ld\"]}]}],null,\"$Le\"]}],{},null,false]},null,false]},null,false]},null,false],\"$Lf\",false]],\"m\":\"$undefined\",\"G\":[\"$10\",[\"$L11\"]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7dd66bdf8a7e5707.js\"],\"OutletBoundary\"]\n14:I[11533,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7dd66bdf8a7e5707.js\"],\"AsyncMetadataOutlet\"]\n16:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7dd66bdf8a7e5707.js\"],\"ViewportBoundary\"]\n18:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7dd66bdf8a7e5707.js\"],\"MetadataBoundary\"]\n19:\"$Sreact.suspense\"\n6:[\"$\",\"p\",null,{\"className\":\"mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line\",\"children\":\"AI code assistants (e.g. GitHub Copilot style tools) are increasingly embedded into development workflows, offering suggestions, rewriting code, generating tests, and more. But while they help speed development, they also open new attack surfaces. Components like context attachment, auto-complete, and direct model invocation can be manipulated maliciously.\"}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"section\",\"2\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4\",\"children\":\"Prompt Injection: The Hidden Threat\"}],[\"$\",\"p\",null,{\"className\":\"mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line\",\"children\":\"‚Ä¢ Direct Prompt Injection: Attackers craft inputs that override system instructions inside the same prompt structure.\\n‚Ä¢ Indirect Prompt Injection: Threat actors insert malicious prompts into external data sources (e.g. public repos, APIs). When a developer feeds that data as ‚Äúcontext‚Äù to a coding assistant, the embedded prompt hijacks the assistant.\\n‚Ä¢ Because LLMs treat all text uniformly, malicious instructions camouflaged inside data can override logic.\"}],[]]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"section\",\"3\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4\",\"children\":\"Misuse of Context Attachment\"}],[\"$\",\"p\",null,{\"className\":\"mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line\",\"children\":\"Many assistants let developers attach external files, URLs, or repositories as context. This boosts relevance but also means if that external content is manipulated, the LLM can be manipulated too. Users may inadvertently feed tainted data, enabling stealthy prompt injection.\"}],[]]}]\n9:[\"$\",\"section\",\"4\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4\",\"children\":\"Harmful Content \u0026 Auto-Completion Abuse\"}],[\"$\",\"p\",null,{\"className\":\"mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line\",\"children\":\"Auto-completion is a double-edged sword. If a prompt or prefix is crafted cleverly, it can coax the model to generate harmful or insecure code‚Äîe.g. insert backdoors, open remote connections, or include data exfiltration logic.\"}],[]]}]\na:[\"$\",\"section\",\"5\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4\",\"children\":\"Direct Model Invocation \u0026 Token Hijacking\"}],[\"$\",\"p\",null,{\"className\":\"mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line\",\"children\":\"Some assistants allow direct calls to the base model from the client side, bypassing constraints. Combined with stolen session tokens (a scenario we call **LLMJacking**), adversaries can supply new system prompts and produce malicious outputs. They may even rent or resell access to legitimate model endpoints.\"}],[]]}]\n"])</script><script>self.__next_f.push([1,"b:[\"$\",\"section\",\"6\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4\",\"children\":\"Mitigation Strategies \u0026 Safe Practices\"}],[\"$\",\"p\",null,{\"className\":\"mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line\",\"children\":\"1. Always Review Generated Code ‚Äî Never blindly trust an AI assistant. Use code reviews, static analysis, and manual auditing before executing suggestions.\\n2. Sanitize \u0026 Vet Context Sources ‚Äî Before attaching external files, repositories, or URLs as context, vet their integrity. Avoid unknown or untrusted sources.\\n3. Limit Auto-Completion in Sensitive Modules ‚Äî Disable or restrict autocomplete features when working in cryptography, authentication, or networking modules.\\n4. Enforce Usage Boundaries ‚Äî Use prompts that clearly separate system instructions from user inputs. Set usage policies for dynamic system prompts.\\n5. Token \u0026 Access Protection ‚Äî Secure session tokens and API keys. Ensure they‚Äôre not exposed to client-side code, and rotate them frequently to mitigate misuse.\\n6. Adopt Defense-in-Depth for AI Tools ‚Äî Combine sandboxing, runtime checks, and anomaly detection to monitor for unexpected behavior triggered by AI outputs.\"}],[]]}]\n"])</script><script>self.__next_f.push([1,"c:[\"$\",\"section\",\"7\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold mb-3 border-l-4 border-blue-600 pl-4\",\"children\":\"Conclusion \u0026 Future Outlook\"}],[\"$\",\"p\",null,{\"className\":\"mb-3 text-gray-700 leading-relaxed text-justify whitespace-pre-line\",\"children\":\"As AI coding assistants become more powerful and ubiquitous, attackers will continue finding new vectors‚Äîespecially via prompt injection and context manipulation. The balance between convenience and security is delicate. Developers must remain vigilant, continuously refine their practices, and never forget: the human in the loop is the last line of defense.\"}],[]]}]\nd:[\"$\",\"div\",null,{\"className\":\"mt-10 pt-4 border-t text-sm text-gray-500\",\"children\":[[\"$\",\"p\",null,{\"className\":\"mb-2\",\"children\":\"Original research and full technical detail available at Unit 42‚Äôs site\"}],[\"$\",\"a\",null,{\"href\":\"https://unit42.paloaltonetworks.com/code-assistant-llms/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-700 underline hover:text-blue-900\",\"children\":\"https://unit42.paloaltonetworks.com/code-assistant-llms/\"}]]}]\ne:[\"$\",\"$L12\",null,{\"children\":[\"$L13\",[\"$\",\"$L14\",null,{\"promise\":\"$@15\"}]]}]\nf:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L16\",null,{\"children\":\"$L17\"}],null],[\"$\",\"$L18\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$19\",null,{\"fallback\":null,\"children\":\"$L1a\"}]}]}]]}]\n11:[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/bf6a6db65f82309d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]\n"])</script><script>self.__next_f.push([1,"17:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n13:null\n"])</script><script>self.__next_f.push([1,"1b:I[27201,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/7dd66bdf8a7e5707.js\"],\"IconMark\"]\n15:{\"metadata\":[[\"$\",\"link\",\"0\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L1b\",\"1\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"1a:\"$15:metadata\"\n"])</script></body></html>